[Hc's blog](https://hc1023.github.io/)

[Hc's blog: R与统计学习基础](https://hc1023.github.io/2021/06/13/Statistical-learning/)

[机器学习与人工智能 浙江大学 中国大学MOOC(慕课)](https://www.icourse163.org/course/ZJU-1206689820)

[TOC]

# 深度学习

2006年，多伦多大学Geoffrey Hinton教授提出真正意义上的“深度”神经网络（deep neural network，DNN），使网络层数达到7层，从而掀起了第二次机器学习热潮——“深度学习”。

深度学习网络即含有多个隐层，因而能学习更复杂的特征。

- 训练误差和测试误差
- 泛化能力：评价模型在其它数据集上的适应能力，一般可以通过测试误差来评估
- 欠拟合（在训练集和测试集表现均较差）和过拟合（训练集上表现较好，测试集表现较差）

## 欠拟合与过拟合

避免欠拟合：

- 增加训练次数；
- 改变网络结构，如增加网络的深度和每一个隐藏层神经元的个数等；
- 调整学习速率。

避免过拟合：

- 数据增强（Data Augmentation），从数据源头获取更多数据；
- 使用合适的模型：减少网络的层数、神经元个数等；
- 正则化；
- Dropout；
- 限制训练时间；或者通过评估测试，提前终止训练。

**正则化方法**：通过引入额外的新信息来解决机器学习中过拟合问题的一种方法，用惩罚保持模型简单性。

以L2正则项为例，我们在代价函数中加上权值的L2正则项
$$
\begin{gathered}
J_{1}=J+\frac{\lambda}{2} \sum_{l} \sum_{i, j}\left(W_{i j}^{(l)}\right)^{2}, \\
\frac{\partial J_{1}}{\partial W_{i j}^{(l)}}=\frac{\partial J}{\partial W_{i j}^{(l)}}+\lambda W_{i j}^{(l)},\\
W_{i j}^{(l)} \leftarrow W_{i j}^{(l)}-\alpha\left(\frac{\partial J}{\partial W_{i j}^{(l)}}+\lambda W_{i j}^{(l)}\right)=(1-\alpha \lambda) W_{i j}^{(l)}-\alpha \frac{\partial J}{\partial W_{i j}^{(l)}}.
\end{gathered}
$$

让参数尽可能小，从而避免过拟合，具有较强抗扰动能力。

**Dropout方法**：在深度学习的训练过程中，对于神经网络单元，按照一定的概率将其暂时从网络中丢弃。

减少了神经元之间复杂的共适应关系，因为减少了两个神经元每次在网络中同时出现的概率，可以打破隐含节点的的固定关系，权值更新不再依赖于它们的共同作用，防止某些特征仅仅在其它特定特征下才有效果的情况，迫使网络去深度学习更加鲁棒性的特征。

而且，由于每次训练存在Dropout，每一次都可能训练一个不同的网络，预测值是多个网络的平均值，从而有效避免过拟合。

## 问题

**梯度下降法**

- 由于网络存在非线性函数，函数非凸，易落入局部最优；
- 而且网络参数量巨大，梯度下降法几乎难以达到全局最优；
- 高维数据中，鞍点存在概率远大于极值点概率，鞍点处梯度为零，所以容易停留在鞍点，现有许多自适应的优化算法如Adam可以解决这个问题。

**梯度消失和梯度爆炸**

反向传播过程中，梯度包含激活函数的导数：
$$
\quad \delta^{(N)}=\left(a^{(N)}-\hat{y}\right) \odot \sigma^{\prime}\left(z^{(N)}\right),
$$
$$
\delta^{(l)}=\left(W^{(l)}\right)^{T} \delta^{(l+1)} \odot \sigma^{\prime}\left(z^{(l)}\right),
$$
$$
\frac{\partial J}{\partial W_{i j}^{(l)}}=\delta_{i}^{(l+1)} a_{j}^{(l)},
$$
$$
\frac{\partial J}{\partial b_{i}^{(l)}}=\delta_{i}^{(l+1)},
$$
导致梯度消失$\nabla_{\theta} J(\theta) \rightarrow 0$，或者梯度爆炸 $\nabla_{\theta} J(\theta) \rightarrow \infty$.

**避免梯度消失和梯度爆炸的方法**

- 预训练加微调：对网络的每一层单独训练，训练完成后对整个网络做微调；
- 梯度裁剪，给定梯度的上下阈值；
- 使用relu，leakrelu，elu等激活函数：

$$
\text{relu}(x)=\max\{0,x\},\ \text{leakrelu}(x)=\max\{x,kx\},\ \text{elu}(x)=\max\{x,\alpha(e^x-1)\};
$$

- 批规范化（Batch Normalization）；
- 使用残差网络结构；
- 采用带记忆的网络结构，例如长短时记忆网络LSTM。

## 卷积神经网络

全连接的神经网络在应用于图像处理时，会导致一个严重的参数数量膨胀问题，所需计算量急速增加。这不仅使得整体算法效率低下，还会导致过拟合，因此全连接神经网络在处理图像处理、语音识别这类问题时无法获得较好的效果。

卷积在数学中的定义

- 连续情况卷积

设 $f(x)$ 和 $g(x)$ 是 $\mathbb{R}$ 上的两个可积函数，称积分函数
$$
s(x):=\int_{-\infty}^{\infty} f(y) g(x-y) \mathrm{d} y
$$
为 $f$ 和 $g$ 的卷积，记为 $s(x)=(f * g)(x)$.

- 离散情况卷积

设 $f(n)$ 和 $g(n)(n=-\infty, \cdots, \infty)$ 是两个欧散宁列, 称
$$
s(n):=\sum_{i=-\infty}^{\infty} f(i) g(n-i)
$$
为 $f$ 和 $g$ 的济散圈积，仍记为 $s(n)=(f * g)(n)$.

- 高维离散情况卷积，例如二维

$$
S(m, n)=(f * g)(m, n)=\sum_{i=-\infty}^{\infty} \sum_{j=-\infty}^{\infty} f(i, j) g(m-i, n-j).
$$

神经网络中的卷积通常为离散卷积，也称为互相关

$$
S(m, n)=(I * K)(m, n)=\sum_{i} \sum_{j} I(m+i, n+j) K(i, j),
$$
其中$K$为卷积核（也成为滤波器filter），输出值$S(m,n)$为特征映射（feature map）。

在图像处理领域，卷积操作被广泛应用。不同卷积核可以提取不同的特征，例如边沿、线性、角等特征。在卷积神经网络中，通过卷积操作可以提取不同级别（简单或复杂）的图像特征。

卷积层的输出通常由深度（depth）、步幅（stride）和补零（zero-padding）三个量来控制。

- 深度：卷积核的个数。
- 步幅：控制同一深度里特征图的两个相邻单元所对应输入区域之间的距离。
- 补零：通过在输入单元周围补零来改变输入单元的整体大小，从而控制输出单元空间大小。

卷积层具有稀疏交互、参数共享、等变表示三个显著特点。使得卷积层具有全局性的同时，极大减少了参数个数，提升学习效率。

池化层也称为降采样（down-sampling），也是一种特征提取的局部操作。池化层的输入一般来源于上一个卷积层的输出，经过池化层后可以非常有效地缩小参数矩阵的尺寸。从而减少后面的卷积层或者全连接层中的参数数量。最常用的池化操作有最大池化（max pooling）和平均池化（mean pooling），在小尺度上对数据做聚合统计。

卷积神经网络架构一般为卷积层叠加池化层，重复，最后全连接层。

举例：LeNet-5卷积神经网络，包括7层，卷积层+池化层+卷积层+池化层+3层全连接层

[LeNet-5 original paper](http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf)











