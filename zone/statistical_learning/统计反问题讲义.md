# 统计反问题

[TOC]

## 1 介绍

- 对于统计学家来说，反问题是推理或估计问题。
- 数据数量是有限的，并且包含误差，未知参数通常是无限维的。
- 数据仅与未知数间接相关。
- 统计反问题：统计估计和推断问题的反问题，标准统计概念，例如偏差，方差，均方误差，可识别性，一致性，效率和各种形式的最优性bias, variance, mean-squared error, identifiability, consistency, efficiency and various forms of optimality适用于反问题。

### 1.1 正向问题

模型集合用具有特定结构的$\Theta$表示，例如，它是实可分巴拿赫空间$\mathcal{T}$的凸子集。

从模型索引到观测量概率分布的正向映射$\theta \in \Theta \mapsto \mathbb{P}_{\theta}$

这个映射可以有合理的分析性质，例如连续性。

数据的可能概率分布类别表示为 $\mathcal{P}=\left\{\mathbb{P}_{\theta}: \theta \in \Theta\right\}$

**线性正向问题**

 $\mathbb{P}_{\theta}$ 是 $K \theta+\epsilon$ 的概率分布，其中$K$ 是线性算子，$\epsilon$ 是分布不依赖于$\theta$的随机变量。

$g(\theta)$ 为模型 $\theta \in \Theta$ 的参数，其可以是恒等变换。

### 1.2 反问题

观察来自未知分布$\mathbb{P}_{\theta}$数据$X$，并根据$\theta \in \Theta$一些已知条件学习 $\theta$，例如估计参数 $g(\theta)$。

 $\Theta$ 包括至少两个点

### 1.3 工具

泛函分析，凸分析，优化理论，非光滑分析，逼近理论，调和分析和测度论等。

## 2 反问题中的可识别性和一致性

- $\Theta$ 是可分巴拿赫空间 $\mathcal{T}$ 的非空子集。 
- $\Theta$ 代表可能的理论——产生观测量$X$的模型 $\theta$。
- $X$ 在任意可分度量 $\mathcal{X}$中取值，在我们的例子中 $\mathcal{X} = \mathbb{R}_n$ 且 $X = \{X_j \}^n_{j
  =1}$。
- $\mathcal{F}$ 是$\mathcal{X}$上的$\sigma$-代数，令  $\mathcal{P}=\left\{\mathbb{P}_{\theta}: \theta \in \Theta\right\}$是定义在$\mathcal{F}$ 上的概率测度。
- 若理论 $\theta \in \Theta$ 是真的，那么$X$的概率分布是$\mathbb{P}_{\theta}$，即$X \sim \mathbb{P}_{\theta}$ 。
- 每一个可能的 $\theta \in \Theta$ 产生一个$X$的概率分布；许多不同的 $\theta$ 可能产生$X$相同的概率分布。在应用数学领域称其为非唯一性问题，在统计领域称其为非可识别性问题。
- **反问题：希望从$X$中学习$\theta$。**
- 称四元组$(\Theta, \mathcal{P}, \mathcal{X}, \mathcal{F})$ 为一个$\Theta$索引的统计实验。

参数关于 $\theta$ 的特征，是我们想学习的。参数是关于 $\theta$ 的连续映射 $g: \Theta \rightarrow \mathcal{G}$，其中 $\mathcal{G}$ 是一个可分度量空间。（限制在连续映射也不是必须的）进一步 $g(\Theta)$ （因而 $\Theta$ ）包含至少两个点，如果$g$在$\Theta$上是常数，则$g(\theta)$已知无需任何观测值。如果我们想完全地估计 $\theta$ ，可以取 $\mathcal{G}=\mathcal{T}$。我们也许会对 $\theta$在低维度的投影更感兴趣，例如$\theta$的范数或其它函数或泛函。

**可识别性**
称参数 $g(\theta)$ 是可识别的如果对于所有的$\theta_{1}, \theta_{2} \in \Theta$，有
$$
\left\{g\left(\theta_{1}\right) \neq g\left(\theta_{2}\right)\right\} \Longrightarrow\left\{\mathbb{P}_{\theta_{1}} \neq \mathbb{P}_{\theta_{2}}\right\}
$$
也就是说，一个参数是可识别的如果参数的变化总能导致数据概率分布的变化。在大部分反问题中，$g(\theta)=\theta$是不可识别的。进一步在大部分线性反问题（后面定义）中，大部分关于 $\theta$的线性泛函是不可识别的，这里提供了关于线性反问题中参数可识别性的一些结论。

### 2.1 估计量 estimator （决策规则）

**（随机）决策规则**
$$
\begin{array}{l}
\delta: \mathcal{X} \rightarrow \mathcal{M}_{1}(\mathcal{A}) \\
x \mapsto \delta(x)(\cdot)
\end{array}
$$
是一个从$\mathcal{X}$ 到可分度量空间 $\mathcal{A}$ （行动空间）的概率分布$\mathcal{M}_{1}(\mathcal{A})$ 的可测映射，其中概率分布是定义在$\mathcal{A}$上Borel $\sigma$ -代数的子$\sigma$ -代数。（从 $\mathcal{X}$ 到$\mathcal{A}$上测度集合的映射$\delta$是可测的，如果对于任意 $\theta \in \Theta$，$\delta(x)(A)$是一个在任意Borel集$A \subseteq \mathcal{A}$上关于$x$为$\mathbb{P}_{\theta}$-可测的函数）

**非随机决策规则**
对每个 $x \in \mathcal{X}$， 赋值一个单位质点$a=a(x) \in \mathcal{A} $ 。考虑随机决策规则在证明以及偶尔在实际操作中确实是有用的，但实际上，它们是非随机决策规则的凸组合。为便于记号，一个非随机决策规则常被记作 $\mathcal{A}$ -valued 函数而不是 $\mathcal{M}_{1}(\mathcal{A})$ -valued。

**（非随机）估计量**
（非随机）估计量$g(\theta)$是一个（非随机）决策规则，其中行动空间$\mathcal{A}$是空间$\mathcal{G}$中所有可能的参数取值。常见的参数估计量$g(\theta)$的记号为$\hat{g}$。与上同样，一个非随机估计量被写作 $\mathcal{G}$ -valued 函数而不是$\mathcal{M}_{1}(\mathcal{G})$ -valued。

**随机估计量举例**
估计硬币头朝上的概率 $p$ ，已知先验概率 $p=1 / 3$或 $2 / 3$。抛$10$次硬币并观测硬币头朝上的总次数$X$。一个合理的估计量$\hat{p}(X)$可以如下定义：令$W$是以$1 / 2$概率为$0$且$1 / 2$概率为$1$并独立于$X$的随机变量。定义
$$
\hat{p}(X)=\left\{\begin{array}{lll}
1 / 3, & X<5 \\
1 / 3, & X=5 & \text { and } \quad W=0 \\
2 / 3, & X=5 & \text { and } \quad W=1 \\
2 / 3, & X>5
\end{array}\right.
$$
这个估计量只会返回 $p$的可能估计值，但是实际上当数据没有偏好时投掷一枚均匀的硬币去决定使用哪个可能的值。



#### 2.1.1 平均距离误差和偏差

**衡量估计量的表现**

有许多常用的测度去评估估计量可能的表现，其中两个最简单的是**平均距离误差**和**平均距离偏差**。简单化起见，我们讲注意力放在非随机估计量上。令 $d_{\mathcal{G}}(\cdot, \cdot)$ 为 $\mathcal{G}$ 上的度量。估计量 $\hat{g}$ 和参数 $g(\theta)$ 在 $\theta$ 处的**平均距离误差**为
$$
\operatorname{MDE}_{\theta}(\hat{g}, g)=\mathbb{E}_{\theta}[d(\hat{g}, g(\theta))]
$$
其为估计量与参数的距离当模型为$\theta$时的期望。因为参数空间 $\mathcal{G}$ 是一个度量空间，且度量在 $\mathbb{R}^{+}$中取值，因此平均距离误差总是良定义的。当度量是由范数定义时，平均距离误差又被称作平均范数误差mean norm error (MNE)。 当 $\mathcal{G}$ 是一个希尔伯特空间有范数 $\|\cdot\|$ ，则其**均方误差**mean squared error (MSE)为
$$
\operatorname{MSE}_{\theta}(\hat{g}, g)=\mathbb{E}_{\theta}\left[\|\hat{g}-g(\theta)\|^{2}\right]
$$
当 $\mathcal{G}$ 是一个巴拿赫空间，我们定义$\hat{g}$在 $\theta$处的**偏差**为
$$
\operatorname{bias}_{\theta}(\hat{g}, g)=\mathbb{E}_{\theta}[\hat{g}-g(\theta)]
$$
若这期望是良定义的。

若 $\operatorname{bias}_{\theta}(\hat{g}, g)=0$，我们称$\hat{g}$对于$g$在$\theta$是**无偏**的。

> **偏差**是系统**误差**，偶然的**误差**在每一次测量中都不同，但是**偏差**会使得测量值偏向同一反向。

若在一个反问题中，存在估计量$\hat{g}$ 对于$g$是无偏的 （称$g$是$U$ -estimable ），那么 $g$ 自然是可识别的。
令 $\bar{g}_{\theta} \equiv \mathbb{E}_{\theta}[\hat{g}]$，那么$\operatorname{bias}_{\theta}(g)=\bar{g}_{\theta}-g(\theta)$。当 $\mathcal{G}$ 是Hilbertian，我们定义估计量$\hat{g}$的**方差**为
$$
\operatorname{Var}_{\theta}(g) \equiv \mathbb{E}_{\theta}\left[\left\|\hat{g}-\bar{g}_{\theta}\right\|^{2}\right]
$$

使用射影定理将均方误差MSE分解为两项的和，$\hat{g}$ 的方差和$\hat{g}$的偏差的范数的平方。即
$$
\begin{aligned}
\mathbb{E}_{\theta}\left[\|\hat{g}-g(\theta)\|^{2}\right] &=\mathbb{E}_{\theta}\left[\left\|\hat{g}-\bar{g}_{\theta}\right\|^{2}\right]+\left\|\bar{g}_{\theta}-g(\theta)\right\|^{2} \\
&=\operatorname{Var}_{\theta}(\hat{g})+\left\|\operatorname{bias}_{\theta}(\hat{g})\right\|^{2}
\end{aligned}
$$
平均距离误差和均方误差都可以作为风险函数。



#### 2.1.2 估计量一致性

在实际问题中，数据数量是有限的并且经常是固定的。 然而，至少在概念上，通常有可能将特定问题嵌入假设的hierarchical序列中，在该假设的hierarchical序列中，进行更多类似类型的实验或进行更多测量。如果可以收集更多数据，就可以以任意精度估算参数，那是极好的。

关注更自然的情形：对于数据空间$n$ th problem $\mathcal{X}_{n}$，所有的问题都有同样的索引空间 $\Theta$以及同样的参数 $g$。进一步，问题序列可以被嵌入，意思是 $\mathcal{X}_{m}$ 是$\mathcal{X}_{n}$的一个Cartesian factor，其中$m \leqslant n$ （例如， $\mathcal{X}_{m}=\mathbb{R}^{m}$ 而 $\mathcal{X}_{n}=\mathbb{R}^{n}$），$m$ th problem $\mathcal{X}_{m}$ 的概率测度$\mathbb{P}_{\theta, m}$ ，为 $\mathcal{X}_{n}$上概率测度$\mathbb{P}_{\theta, n}$ 的边缘测度。因此，问题的不同之处在于区别有多少可用数据量。若对于任何参数值，当使用更多的数据，参数的估计值都能收敛到它的真实值，那么称其估计值序列为一致的。

**注记2.2.** 记$d_{\mathcal{G}}$为$\mathcal{G}$上的度量。

**定义2.3. 一致性** 给定正向问题一个内嵌序列$\left\{\left\{\mathbb{P}_{\theta, n}: \theta \in \Theta\right\}\right\}_{n=1}^{\infty}$  和参数$g: \Theta \rightarrow \mathcal{G}$，一个非随机估计量序列$\left\{\hat{g}_{n}\right\}_{n=1}^{\infty}, g_{n}: \mathcal{X}_{n} \rightarrow \mathcal{G}$ 对于$g$是**一致**的如果对任一 $\theta \in \Theta$ 以及任一$g(\theta) \in \mathcal{G}$的邻域$U$，有
$$
\lim _{n \rightarrow \infty} \mathbb{P}_{\theta, n}\{\hat{g} \notin U\}=0。
$$
以上定理表明，参数 $g$ 是一致可估计的如果存在一个估计量序列对于$g$是一致的。如果$g$是恒等映射且 $g(\theta)=\theta$ 在$\Theta$上的某种拓扑（不一定是$\mathcal{T}$上的范数拓扑）是一致可估计的，则称这个模型是一致可估计的。



### 2.2 线性正向问题

**注记2.4.** 令 $\mathcal{T}$ 是一个可分巴拿赫空间。那么 $\mathcal{T}^{*}\left(\mathcal{T}^{* *}\right)$ 定义为$\mathcal{T}$的范数对偶（范数第二对偶）, 且pairing between $\mathcal{T}^{*}$ and $\mathcal{T}$ (between $\mathcal{T}^{* *}$ and $\mathcal{T}^{*}$ ) 记为$\langle\cdot, \cdot\rangle: \mathcal{T}^{*} \times \mathcal{T} \rightarrow \mathbb{R}\left(\langle\langle\cdot, \cdot\rangle\rangle: \mathcal{T}^{* *} \times \mathcal{T}^{*} \rightarrow \mathbb{R}\right)$。在 $\mathcal{T}, \mathcal{T}^{*}$ 和 $\mathcal{T}^{* *}$ 上的范数分别记为$\|\cdot\|,\|\cdot\|_{*}$ 和 $\|\cdot\|_{* *}$。 

**定义2.5.** 一个正向问题是线性的如果以下条件成立
（1）$\Theta$ 是可分巴拿赫空间 $\mathcal{T}$的一个子集。
（2）对于$\mathcal{T}^{*}$中元素构成的特定序列$\left\{\kappa_{j}\right\}_{j=1}^{n}$ ，数据$X=\left\{X_{j}\right\}_{j=1}^{n}$成立
$$
X_{j}=\left\langle\kappa_{j}, \theta\right\rangle+\epsilon_{j}, \quad \theta \in \Theta
$$
且 $\epsilon=\left\{\epsilon_{j}\right\}_{j=1}^{n}$ 是随机误差构成的向量，它的概率分布**不**依赖于 $\theta$ 。（因而 $\mathcal{X}=\mathbb{R}^{n}$。 ）

泛函$\left\{\kappa_{j}\right\}_{j=1}^{n}$是线性正向问题的'representers'或'data kernels' 。它的分布 $\mathbb{P}_{\theta}$ 是 $X$的概率分布，而 $\mathcal{P}$是$\theta$ 取遍 $\Theta$的分布构成的集合。 往往 $\operatorname{dim}(\Theta)=\infty$，或至少是$n<\operatorname{dim}(\Theta)$，因此估计$\theta$是一个**欠定问题**。定义
$$
K: \Theta \rightarrow \mathbb{R}^{n} \quad \theta \mapsto\left\{\left\langle\kappa_{j}, \theta\right\rangle\right\}_{j=1}^{n}
$$
则可以将定义2.5中的式子写作
$$
X=K \theta+\epsilon, \quad \theta \in \Theta
$$
利用数据 $X=K \theta+\epsilon$ 以及$\theta \in \Theta$有关的知识，去估计或对参数$g(\theta)$作推断是一个线性反问题。在线性反问题中，数据$X$的概率分布仅通过$K\theta$依赖于模型 $\theta$ ，所以如果有两个点$\theta_{1}, \theta_{2} \in \Theta$ 满足 $K \theta_{1}=K \theta_{2}$ 但是$g\left(\theta_{1}\right) \neq g\left(\theta_{2}\right)$，那么$g(\theta)$是不可识别的。我们继续学习关于$K, \Theta$ 和 $g$ 的一些条件，它们可以控制$K \theta$是否可以在$\Theta$上决定$g(\theta)$。 

### 2.3 线性反问题中线性参数的可识别性

考虑线性正向问题， $\#(\Theta) \geqslant 2$。令 $\left\{g_{i}\right\}_{i=1}^{m}$ 是在 $\Theta$ 上线性的（但不必有界）泛函集合：对于 $a_{1}, a_{2} \in \mathbb{R}$ 和 $\theta_{1}, \theta_{2} \in \Theta$ ，有 $g_{i}\left(a_{1} \theta_{1}+a_{2} \theta_{2}\right)=a_{1} g_{i}\left(\theta_{1}\right)+a_{2} g_{i}\left(\theta_{2}\right), i=1,2, \ldots, m$，当 $a_{1} \theta_{1}+a_{2} \theta_{2} \in \Theta$。这节将解决从数据 $X=K \theta+\epsilon$ 对线性参数向量进行估计
$$
g(\theta) \equiv\left\{g_{i}(\theta)\right\}_{i=1}^{m}。
$$
线性正向问题中，由data kernels线性组合构成的线性泛函在参数估计中起到了决定性的作用。令 $\Lambda$ 是实元素$\lambda_{i j}$组成的 $m \times n$ 矩阵。我们定义
$$
\begin{array}{l}
\Lambda \cdot K: \mathcal{T} \rightarrow \mathbb{R}^{m} \\
t \mapsto\left(\sum_{j=1}^{n} \lambda_{1 j}\left\langle\kappa_{j}, t\right\rangle, \sum_{j=1}^{n} \lambda_{2 j}\left\langle\kappa_{j}, t\right\rangle, \ldots, \sum_{j=1}^{n} \lambda_{m j}\left\langle\kappa_{j}, t\right\rangle\right)
\end{array}
$$
实数参数可识别的以下必要条件扩展了Backus and Gilbert定理，它所处理的参数比刚刚描述的线性参数更通用。 注意，当且仅当向量值参数的每个分量都是可识别的时，向量值参数才可识别，因此考虑实值参数就足够了。 回忆引理A.4，若 $Y$ 是一个随机 $n$ -vector 而 $a \in \mathbb{R}, a \neq 0$，那么$Y$ 的概率分布与$a+Y$不同，因此在一个线性反问题中，$K \theta_{1} \neq K \theta_{2}$ iff $\mathbb{P}_{\theta_{1}} \neq \mathbb{P}_{\theta_{2}}$。这可以得出参数$g$是可识别的当且仅当 $g\left(\theta_{1}\right) \neq g\left(\theta_{2}\right)$ 可以推出 $K \theta_{1} \neq K \theta_{2}$ 对任何 $\theta_{1}, \theta_{2} \in \Theta$。

**引理A.4（变换的可识别性）.** 令 $Y$ 是一个随机 $n$ -vector。给定一个常数 $a \in \mathbb{R}^{n}$，$a \neq 0$，则$Y$的概率分布与 $a+Y$ 不同。

**证明.** 使用Fourier methods这是显然的。注意到 $\mathbb{E}[\exp (\mathrm{i} z \cdot(a+Y))]$ $=\exp (\mathrm{i} z \cdot a) \mathbb{E}[\exp (\mathrm{i} z \cdot Y)]$ 对于 $z \in \mathbb{R}^{n}$ 成立。函数 $z \mapsto \mathbb{E}[\exp (\mathrm{i} z \cdot Y)]$ 是连续的，且在 $0$ 处取值为 $1$，因此在 $0$ 附近一个邻域中是非$0$的。因此对于某个 $z \in \mathbb{R}^{n}$ 有 $\mathbb{E}[\exp (\mathrm{i} z \cdot(a+Y))] \neq$ $\mathbb{E}[\exp (\mathrm{i} z \cdot Y)]$ ，由Fourier唯一性得证结论。

**定理2.6.** 令 $g: \Theta \rightarrow \mathbb{R}$ 是一个可识别的实值参数。设存在一个非空对称凸集 $\bar{\Theta} \subseteq \mathcal{T}$ 满足
(i) $\bar{\Theta} \subseteq \Theta$
(ii) $g(-\theta)=-g(\theta), \theta \in \bar{\Theta}$,
(iii) $g\left(a_{1} \theta_{1}+a_{2} \theta_{2}\right)=a_{1} g\left(\theta_{1}\right)+a_{2} g\left(\theta_{2}\right), \theta_{1}, \theta_{2} \in \bar{\Theta}, a_{1}, a_{2} \geqslant 0, a_{1}+a_{2}=1$ 以及
(iv) $ \sup _{\theta \in \bar{\Theta}}|g(\theta)|<\infty$
那么存在一个$1 \times n$矩阵$\Lambda$满足 $g$ 在 $\bar{\Theta}$ 的限制是 $\Lambda \cdot K$ 在 $\bar{\Theta}$ 上的限制。

**证明.** 不失一般性假设 $\bar\Theta=\Theta$。用 $\Theta$ 张成的 $\mathcal{T}$ 的闭子空间代替 $\mathcal{T}$ ，可以进一步不失一般性假设 $\Theta$ 张成的 $\mathcal{T}$ 的闭子空间即为所有的 $\mathcal{T}$。那么 $g$ 是限制在 $\Theta$ 上为 $\mathcal{T}$ 上的连续线性泛函，可以同样被记为 $g$。

假设这样的矩阵$\Lambda$不存在，那么$g$不是函数$\kappa_{1}, \ldots, \kappa_{n}$的线性组合，由连续性可以知道存在一个有限集$T \subset \Theta$满足$g$限制在 $T$ 上不是函数 $\kappa_{1}, \ldots, \kappa_{n}$ 限制在$T$上的线性组合（我觉得有限维可以取基，无限维可以归纳反证）。也就是说，有限维向量 $\{\langle g, \theta\rangle\}_{\theta \in T}$ 不是向量 $\left\{\left\langle\kappa_{i}, \theta\right\rangle\right\}_{\theta \in T}, 1 \leqslant i \leqslant n$的线性组合。根据有限维线性代数，存在常数$\left\{a_{\theta}\right\}_{\theta \in T}$满足 $\sum_{\theta \in T} a_{\theta}\langle g, \theta\rangle \neq 0$ 且 $\sum_{\theta \in T} a_{\theta}\left\langle\kappa_{i}, \theta\right\rangle=0,1 \leqslant i \leqslant n$。进一步，用$\left\{\gamma a_{\theta}\right\}_{\theta \in T}$ 代替$\left\{a_{\theta}\right\}_{\theta \in T}$ ，且取$\gamma$足够小，我们可以认为 $\sum_{\theta \in T} a_{\theta} \theta \in \Theta$。
观察到
$$
g\left(\sum_{\theta \in T} a_{\theta} \theta \in \Theta\right)=\sum_{\theta \in T} a_{\theta}\langle g, \theta\rangle \neq 0=g(0)
$$
然而
$$
K\left(\sum_{\theta \in T} a_{\theta} \theta \in \Theta\right)=\left(\sum_{\theta \in T} a_{\theta}\left\langle\kappa_{i}, \theta\right\rangle\right)_{i=1}^{n}=0=K(0)
$$
这与可识别性矛盾。
定理2.6可以变得更一般化，例如，假设存在 $\theta_{0} \in \Theta$，对称凸集 $\bar{\Theta} \subseteq \mathcal{T}$，常量$c \in \mathbb{R}$ 以及映射 $\bar{g}: \bar{\Theta} \rightarrow \mathbb{R}$ 满足
(i) $\theta_{0}+\bar{\Theta} \subseteq \Theta$
(ii) $g\left(\theta_{0}+\bar{\theta}\right)=c+\bar{g}(\bar{\theta}), \bar{\theta} \in \bar{\Theta}$
(iii) $\bar{g}(-\bar{\theta})=-\bar{g}(\bar{\theta}), \bar{\theta} \in \bar{\Theta}$
(iv) $\bar{g}\left(a_{1} \bar{\theta}_{1}+a_{2} \bar{\theta}_{2}\right)=a_{1} \bar{g}\left(\bar{\theta}_{1}\right)+a_{2} \bar{g}\left(\bar{\theta}_{2}\right), \bar{\theta}_{1}, \bar{\theta}_{2} \in \bar{\Theta}, a_{1}, a_{2} \geqslant 0, a_{1}+a_{2}=1$ and
(v) $\sup _{\bar{\theta} \in \bar{\Theta}}|\bar{g}(\bar{\theta})|<\infty$
那么存在 $1 \times n$ 矩阵 $\Lambda$满足$g$在$\bar{\Theta}+\theta_{0}$的限制是$\Lambda \cdot K\left(\cdot-\theta_{0}\right)+c$ 在 $\bar{\Theta}+\theta_{0}$上的限制。
定理2.6给出可识别性的一个必要条件，后面给出相应的充分条件。 



**定理2.7.** 假设 $g=\left\{g_{i}\right\}_{i=1}^{m}$ 是一个 $\mathbb{R}^{m}$ -valued参数，可以被写作$\Lambda \cdot K$ 在$\Theta$上的限制，其中$\Lambda$为 $m \times n$矩阵。那么$g$是可识别的。进一步若$\mathbb{E}[\epsilon]=0$，那么统计量$\Lambda \cdot X$是估计量$g$的无偏估计。如果$\epsilon$存在协方差矩阵$\Sigma=\mathbb{E}\left[\epsilon \epsilon^{T}\right]$，那么 $\Lambda \cdot X$在任何$\mathbb{P}_{\theta}$下的协方差矩阵是$\Lambda \cdot \Sigma \cdot \Lambda^{T}$ 。

**证明.** $g$的可识别性可从引理A.4.立刻得出。若$\mathbb{E}[\epsilon]=0$，我们计算
$$
\begin{aligned}
\mathbb{E}_{\theta}[\Lambda \cdot X] &=\mathbb{E}_{\theta}[\Lambda \cdot K \theta+\Lambda \cdot \epsilon] \\
&=\mathbb{E}_{\theta}[\Lambda \cdot K \theta]+\mathbb{E}_{\theta}[\Lambda \cdot \epsilon] \\
&=\Lambda \cdot K \theta+\Lambda \cdot \mathbb{E}_{\theta}[\epsilon] \\
&=\Lambda \cdot K \theta \\
&=g(\theta)
\end{aligned}
$$
所以$\Lambda \cdot X$是估计量$g(\theta)$的无偏估计。另外设 $\epsilon$有协方差矩阵$\Sigma$。可计算
$$
\begin{aligned}
\mathbf{C o v}_{\theta}(\Lambda \cdot X) &=\mathbb{E}\left[\Lambda \cdot \epsilon \cdot \epsilon^{T} \cdot \Lambda^{T}\right] \\
&=\Lambda \cdot \Sigma \cdot \Lambda^{T}
\end{aligned}
$$

**推论 2.8  (the fundamental theorem of Backus and Gilbert).** 令 $\mathcal{T}$ 是希尔伯特空间，令 $\Theta=\mathcal{T}$，$g \in \mathcal{T}=\mathcal{T}^{*}$ 为线性参数且 $\left\{\kappa_{j}\right\}_{j=1}^{n} \subseteq \mathcal{T}^{*} $，则参数$g(\theta)$是可识别的当且仅当存在$1 \times n$矩阵$\Lambda$使得$g=\Lambda \cdot K$ 。 在这种情形下，若$\mathbb{E}[\epsilon]=0$，那么$\hat{g}=\Lambda \cdot X$ 对于$g$是无偏的。另外，如果$\epsilon$有协方差矩阵$\Sigma=\mathbb{E}\left[\epsilon \epsilon^{T}\right]$，那么$\hat{g}$的均方误差MSE是$\Lambda \cdot \Sigma \cdot \Lambda^{T}$。



### 2.4 线性反问题中的一致性

这一节在一个相当一般的情形下讨论了线性反问题中模型$\theta$可以被一致估计的充分条件。

在本节假设观测误差$\epsilon$是一个独立同分布实值随机变量组成的$n$-vector并服从一个常见的分布$\mu$。此处对矩条件$\mu$没有要求。

整个模型是否可以被一致估计取决于考虑的模型空间，$\Theta$中模型的先验条件，已知泛函$K$以及观测误差的概率分布。 我们的结果将根据一组概率测度$\left\{\mathbb{P}_{\theta}: \theta \in \Theta\right\}$的“大小”来界定。

**定义2.9.** 令 $\mu_{a}, a \in \mathbb{R}$，用映射$x \mapsto x+a$来定义$\mu$ 的一个前推，即$\mu_{a}(B)=\mu(B-a)$。用Hellinger distance $\delta(a, b)$定义$\mathbb{R}$上度量
$$
\begin{aligned}
0 \leqslant \delta(a, b) & \equiv\left\{\frac{1}{2} \int\left(\sqrt{\mathrm{d} \mu_{a}}-\sqrt{\mathrm{d} \mu_{b}}\right)^{2}\right\}^{\frac{1}{2}} \\
&=\left\{\frac{1}{2} \int\left(\sqrt{\frac{\mathrm{d} \mu_{a}}{\mathrm{~d}\left(\mu_{a}+\mu_{b}\right)}}-\sqrt{\frac{\mathrm{d} \mu_{b}}{\mathrm{~d}\left(\mu_{a}+\mu_{b}\right)}}\right)^{2} \mathrm{~d}\left(\mu_{a}+\mu_{b}\right)\right\}^{\frac{1}{2}} \leqslant 1
\end{aligned}
$$
为测度$\mu_{a}$与$\mu_{b}$之间的距离。度量$\delta(a, b)$是平移不变量（它依赖于$|a-b|$）。

**定义2.10.** 任给 $\epsilon>0$，度量空间 $(S, \rho)$ 的一个 $\epsilon$-网是一个子集 $R \subseteq S$ 满足对任意 $s \in S$，存在$r \in R$有 $\rho(r, s)<\epsilon$ 。度量空间 $(S, \rho)$是全有界的，如果对于每个 $\epsilon>0$ 存在有限的$\epsilon$-网。 紧性总是意味着全有界，反向在完备空间中反向成立（反向并不总是成立）。

> 伪度量空间是度量空间的一般化，其中两个不同点之间的距离可以为零。 就像每个范数空间都是一个度量空间一样，每个半范数空间都是一个伪度量空间。

**注记2.11.** 给定一个严格正常数序列$\left\{C_{n}\right\}_{n=1}^{\infty}$，定义$\mathcal{T}$上的伪度量$d_{n}, n \in \mathbb{N}$ 
$$
d_{n}\left(x^{\prime}, x^{\prime \prime}\right)=\left\{\frac{1}{C_{n}} \sum_{i=1}^{n} \delta^{2}\left(\left\langle\kappa_{i}, x^{\prime}\right\rangle,\left\langle\kappa_{i}, x^{\prime \prime}\right\rangle\right)\right\}^{\frac{1}{2}}
$$
**定理2.12.** 假设 $\lim _{n} C_{n}=\infty$，存在可数个子集序列 $\Theta_{1} \subseteq \Theta_{2} \cdots \subseteq \Theta$ 满足 $\Theta=\bigcup_{h} \Theta_{h}$ 且 $d_{n}$ 在每一个 $\Theta_{h} \times \Theta_{h}$上一致收敛到$\Theta$上的度量$d$。进一步假设每一个集合 $\Theta_{h}$对于 $d$是全有界的，则模型在$d$-topology中是一致可估的。 

**证明.** 首先设 $d_{n}$ 在 $\Theta$ 上一致收敛到度量 $d$ 且 $\Theta$ 在度量$d$下是一致有界的。对于 $k \in \mathbb{N}$，令 $\left\{\theta_{k, 1}, \ldots, \theta_{k, K_{k}}\right\}$ 为 $\Theta$ 中的有限 $2^{-k}$ -网。

根据Birgé中的结论 （see proposition 3 , section 16.4 of [44]），对每个 $n \in \mathbb{N}$ 以及 $\left(k, \ell^{\prime}\right),\left(k, \ell^{\prime \prime}\right)$ ，存在数 $a>0$ 和 $b>0$ 及在 $\mathbb{R}^{n}$ 上的 $ \{0,1\}$-valued函数 $\psi_{n, k, \ell^{\prime}, \ell^{\prime \prime}}$ 满足性质
$\inf \left\{\mathbb{P}_{\theta}\left\{\psi_{n, k, \ell^{\prime}, \ell^{\prime \prime}}\left(X_{1}, \ldots, X_{n}\right)=1\right\}: d_{n}\left(\theta, \theta_{k, \ell^{\prime}}\right) \leqslant a d_{n}\left(\theta_{k, \ell^{\prime}}, \theta_{k, \ell^{\prime \prime}}\right)\right\}$
$$
\geqslant 1-\exp \left(-b C_{n} d_{n}^{2}\left(\theta_{k, \ell^{\prime}}, \theta_{k, \ell^{\prime \prime}}\right)\right)
$$
和
$\sup \left\{\mathbb{P}_{\theta}\left\{\psi_{n, k, \ell^{\prime}, \ell^{\prime \prime}}\left(X_{1}, \ldots, X_{n}\right)=1\right\}: d_{n}\left(\theta, \theta_{k, \ell^{\prime \prime}}\right) \leqslant a d_{n}\left(\theta_{k, \ell^{\prime}}, \theta_{k, \ell^{\prime \prime}}\right)\right\}$
$$
\leqslant \exp \left(-b C_{n} d_{n}^{2}\left(\theta_{k, \ell^{\prime}}, \theta_{k, \ell^{\prime \prime}}\right)\right)
$$
对每个 $k$ 选择 $N_{k} \in \mathbb{N}$ ，当 $n \geqslant N_{k}$，有 
$$
d_{n}\left(\theta_{k, \ell^{\prime}}, \theta_{k, \ell^{\prime \prime}}\right) \geqslant \frac{1}{2} d\left(\theta_{k, \ell^{\prime}}, \theta_{k, \ell^{\prime \prime}}\right), \quad 1 \leqslant \ell^{\prime} \neq \ell^{\prime \prime} \leqslant K_{k}
$$
对于 $1 \leqslant \ell^{\prime} \leqslant K_{k}$ ，记
$$
L_{k}\left(\ell^{\prime}\right)=\left\{\ell^{\prime \prime}: d\left(\theta_{k, \ell^{\prime}}, \theta_{k, \ell^{\prime \prime}}\right) \geqslant a^{-1} 2^{-(k-2)}\right\}
$$
设
$$
\chi_{k, n, \ell^{\prime}}=\prod_{\ell^{\prime \prime} \in L_{k}\left(\ell^{\prime}\right)} \psi_{n, k, \ell^{\prime}, \ell^{\prime \prime}}\left(X_{1}, \ldots X_{n}\right)
$$
其中若 $L_{k}\left(\ell^{\prime}\right)=\emptyset$ 定义乘积为 $1$。根据构造，若 $n \geqslant N_{k}$ ，$d_{n}\left(\theta, \theta_{k, \ell^{\prime}}\right)<2^{-(k-1)}$，那么
$$
d_{n}\left(\theta, \theta_{k, \ell^{\prime}}\right)<2^{-(k-1)} \leqslant \frac{a}{2} d\left(\theta_{k, \ell^{\prime}}, \theta_{k, \ell^{\prime \prime}}\right) \leqslant a d_{n}\left(\theta_{k, \ell^{\prime}}, \theta_{k, \ell^{\prime \prime}}\right), \quad \ell^{\prime \prime} \in L_{k}\left(\ell^{\prime}\right)
$$
因此有
$$
\begin{aligned}
\mathbb{P}_{\theta}\left\{\chi_{k, n, \ell^{\prime}}=1\right\} & \geqslant 1-\sum_{\ell^{\prime \prime} \in L_{k}\left(\ell^{\prime}\right)} \exp \left(-b C_{n} d_{n}^{2}\left(\theta_{k, \ell^{\prime}}, \theta_{k, \ell^{\prime \prime}}\right)\right) \\
& \geqslant 1-\sum_{\ell^{\prime \prime} \in L_{k}\left(\ell^{\prime}\right)} \exp \left(-b C_{n} 2^{-2} d^{2}\left(\theta_{k, \ell^{\prime}}, \theta_{k, \ell^{\prime \prime}}\right)\right) \\
& \geqslant 1-K_{k} \exp \left(-b C_{n} a^{-2} 2^{-2(k-1)}\right)
\end{aligned}
$$
进一步，对任意 $\ell^{\prime \prime} \in L_{k}\left(\ell^{\prime}\right)$ 我们有
$$
\begin{aligned}
\mathbb{P}_{\theta}\left\{\chi_{k, n, \ell^{\prime \prime}}=1\right\} & \leqslant \exp \left(-b C_{n} d_{n}^{2}\left(\theta_{k, \ell^{\prime}}, \theta_{k, \ell^{\prime \prime}}\right)\right) \\
& \leqslant \exp \left(-b C_{n} 2^{-2} d^{2}\left(\theta_{k, \ell^{\prime}}, \theta_{k, \ell^{\prime \prime}}\right)\right) \\
& \leqslant \exp \left(-b C_{n} a^{-2} 2^{-2(k-1)}\right)
\end{aligned}
$$
若 $\left\{1 \leqslant \ell \leqslant K_{k}: \chi_{k, n, \ell}=1\right\}$是空集，令 $\hat{\theta}_{k, n}$ 为任意点 $\theta_{0} \in \Theta$。否则令$\hat{\theta}_{k, n}=\theta_{k, p(k, n)}$，其中 $p(k, n)=\min \left\{1 \leqslant \ell \leqslant K_{k}: \chi_{k, n, \ell}=1\right\}$。考虑 $\theta \in \Theta$ 且选择 $\theta_{k, \ell^{\prime}}$ 满足 $d\left(\theta, \theta_{k, \ell^{\prime}}\right)<2^{-k}$。由上
$$
\begin{aligned}
\mathbb{P}_{\theta}\left\{d\left(\hat{\theta}_{k, n}, \theta\right)\right.&\left.>a^{-1} 2^{-(k-2)}+2^{-k}\right\} \\
& \leqslant \mathbb{P}_{\theta}\left(\left\{\chi_{k, n, \ell^{\prime}}=0\right\} \cup \bigcup_{\ell^{\prime \prime} \in L_{k}\left(\ell^{\prime}\right)}\left\{\chi_{k, n, \ell^{\prime \prime}}=1\right\}\right)+\mathbf{1}\left\{d_{n}\left(\theta, \theta_{k, \ell^{\prime}}\right) \geqslant 2^{-(k-1)}\right\} \\
& \leqslant 2 K_{k} \exp \left(-b C_{n} a^{-2} 2^{-2(k-1)}\right)+\mathbf{1}\left\{d_{n}\left(\theta, \theta_{k, \ell^{\prime}}\right)-d\left(\theta, \theta_{k, \ell^{\prime}}\right)>2^{-(k-1)}-2^{-k}\right\}
\end{aligned}
$$
现归纳地定义$1=n_{1}<n_{2}<\cdots \in \mathbb{N}$ 为
$$
\begin{aligned}
n_{k+1}=\min \left\{n>n_{k}: 2 K_{k+1}\right.& \exp \left(-b C_{n} a^{-2} 2^{-2 k}\right) \leqslant 2^{-(k+1)} \\
&\left.\sup _{\theta^{\prime}, \theta^{\prime \prime}}\left|d_{n}\left(\theta^{\prime}, \theta^{\prime \prime}\right)-d\left(\theta^{\prime}, \theta^{\prime \prime}\right)\right| \leqslant 2^{-k}-2^{-(k+1)}\right\}
\end{aligned}
$$
并令
$$
\hat{\theta}_{n}=\hat{\theta}_{k, n}, \quad n_{k} \leqslant n<n_{k+1}
$$
显然对每个 $\eta>0$
$$
\lim _{n} \sup _{\theta \in \Theta} \mathbb{P}_{\theta}\left\{d\left(\hat{\theta}_{n}, \theta\right)>\eta\right\}=0
$$
先考虑满足定理条件一般情形下的 $\Theta$ 。可用$\Theta_{h}$ 代替 $\Theta$的角色，则记$\left\{\hat{\theta}_{n}^{h}\right\}_{n=1}^{\infty}$ 为由以上方法构造的估计量序列。归纳地定义 $1=m_{1}<m_{2}<\cdots \in \mathbb{N}$ 满足
$$
m_{k+1}=\min \left\{m>m_{k}: \sup _{\theta \in \Theta_{k+2}} \mathbb{P}_{\theta}\left\{d\left(\hat{\theta}_{p}^{k+2}, \theta\right)>2^{-(k+1)}\right\}<2^{-(k+1)}, \forall p \geqslant m\right\}
$$
并令
$$
\hat{\theta}_{n}=\hat{\theta}_{m}^{k+1}, \quad m_{k} \leqslant m<m_{k+1}
$$
显然对于所有的 $\theta \in \Theta$，序列 $\left\{\hat{\theta}_{n}\right\}_{n=1}^{\infty}$ 依 $\mathbb{P}_{\theta}$ 概率在 $d$ -topology中收敛到 $\theta$ 。

**推论2.17.** 假设 $C_{n}=n$且对于可数个集合序列 $\Theta_{1} \subseteq \Theta_{2} \subseteq \cdots \subseteq \Theta$ 满足 $\Theta=\bigcup_{h} \Theta_{h}$ ，它们对于范数拓扑中是紧的，且 $d_{n}$ 在每一个 $\Theta_{h} \times \Theta_{h}$上逐点收敛到 $\Theta$上的度量 $d$。进一步假设 $\mu$ 是绝对连续的，且 $\sup _{i}\left\|\kappa_{i}\right\|_{*}<\infty$，则模型在 $d$-topology中是一致可估的。 

**证明** 我们首先证明 $d_{n}$ 在每个 $\Theta_{h}$ 中一致收敛到 $d$ 。观察到
$$
d_{n}\left(x^{\prime}, x^{\prime \prime}\right) \leqslant \sup \left\{\delta(0, t):|t| \leqslant \sup _{i}\left\|\kappa_{i}\right\|_{*}\left\|x^{\prime}-x^{\prime \prime}\right\|\right\}
$$
且注意到
$$
\left|d_{n}\left(y^{\prime}, y^{\prime \prime}\right)-d_{n}\left(z^{\prime}, z^{\prime \prime}\right)\right|=\left|d_{n}\left(0, y^{\prime}-y^{\prime \prime}\right)-d_{n}\left(0, z^{\prime}-z^{\prime \prime}\right)\right| \leqslant d_{n}\left(y^{\prime}-y^{\prime \prime}, z^{\prime}-z^{\prime \prime}\right)
$$
因为 $\mu$ 是绝对连续的，
$$
\lim _{t \rightarrow 0} \delta(0, t)=0
$$
函数序列 $\left\{d_{n}\right\}_{n=1}^{\infty}$ 因此在范数诱导的度量下在 $\Theta_{h} \times \Theta_{h}$ 中是等度连续的，则由紧性和逐点收敛知这是一致收敛的（见下补充证明）。

根据定理2.12的条件，我们只需要证明每个 $\Theta_{h}$ 在 $d_n$下是全有界。由于 $\Theta_{h}$ 在范数拓扑中是紧的，则在范数诱导的度量中是全有界的。于是再由(2.48)和(2.50)得到$d$是全有界的。（因为$d$可以被范数控制）

**补充证明.**  若$\{f_n\}$在紧度量空间$K$上等度连续、逐点收敛，则一致收敛。

**证明.** 只需证明对任意$\epsilon > 0$，存在$n_0=n_0(\epsilon)$，满足对所有$n\geq n_0$对所有$x\in K$有
$$
\left|f_{n}(x)-f(x)\right|<\varepsilon
$$

由$\left\{f_{n}\right\}$ 是等度连续的，则对$\forall n\in\mathbb N$，存在 $\delta>0$，当 $d(x, y)<\delta$时有
$$
\left|f_{n}(x)-f_{n}(y)\right|<\frac{\varepsilon}{3}
$$
令 $n \rightarrow \infty$ ，有
$$
|f(x)-f(y)| \leq \frac{\varepsilon}{3}
$$
由于 $K$ 是紧的，存在 $k \in \mathbb{N}$ 以及 $z_{1}, \ldots, z_{k} \in K$满足
$$
K \subset B\left(z_{1}, \delta\right) \cup \cdots \cup B\left(z_{k}, \delta\right)
$$
由于 $f_{n}\left(z_{j}\right) \rightarrow f\left(z_{j}\right)$对$j=1, \ldots, k$，存在$n_{0}$，当$n \geq n_{0}$有
$$
\left|f_{n}\left(z_{j}\right)-f\left(z_{j}\right)\right|<\frac{\varepsilon}{3},\ j=1, \ldots, k
$$
由此任取 $x \in K$ ，存在 $j \in\{1, \ldots, k\}$使得$x \in B\left(z_{j}, \delta\right)$ ，则当$n \geq n_{0}$有
$$
\begin{array}{c}
\left|f_{n}(x)-f(x)\right| \leq\left|f_{n}(x)-f_{n}\left(z_{j}\right)\right|+\left|f_{n}\left(z_{j}\right)-f\left(z_{j}\right)\right|+\left|f\left(z_{j}\right)-f(x)\right|<\frac{\varepsilon}{3}+\frac{\varepsilon}{3}+\frac{\varepsilon}{3}=\varepsilon
\end{array}
$$

**例子2.18.** 令 $\mathcal{T}=C([0,1])$， 巴拿赫空间$[0,1]$上的连续函数，supremum范数。对于 $0<\alpha \leqslant 1$，令 $\Theta$ 为满足Hölder condition of order $\alpha$的函数集合，即$\Theta$ 为 $x \in C([0,1])$ 满足
$$
\sup \left\{|x(s)-x(t)| /|s-t|^{\alpha}, 0 \leqslant s \neq t \leqslant 1\right\}<\infty
$$
根据 Arzela-Ascoli 定理，集合 $\Theta$ 时可数个在范数拓扑中紧的集合的并。（见下补充说明）令 $\mu$ 为绝对连续概率测度且取 $C_{n}=n$。固定一个无理数  $\zeta$ 并令 $\kappa_{i}$ 为取值在$i\zeta$的泛函，即$\left\langle\kappa_{i}, x\right\rangle=x(\mathrm{i} \zeta-\lfloor\mathrm{i} \zeta\rfloor)$。由Kronecker-Weyl equidistribution 定理，对于每一对$x^{\prime}, x^{\prime \prime} \in C([0,1])$，有
$$
\lim _{n} d_{n}\left(x^{\prime}, x^{\prime \prime}\right)=\left\{\int_{0}^{1} \delta^{2}\left(x^{\prime}(t), x^{\prime \prime}(t)\right) \mathrm{d} t\right\}^{\frac{1}{2}}=d\left(x^{\prime}, x^{\prime \prime}\right)
$$
则从推论2.17得模型在$d$-拓扑中是一致可估计的。

**补充说明**
令
$$
H=\left\{x \in C[0,1]: \forall s, t \in[0,1]:|x(s)-x(t)| \leq b|s-t|^{\alpha}\right\}
$$
其中 $C[0,1]$ 为 $[0,1]$ 上的连续函数且有$x(0)=0$（这不是本质的）和$\alpha \in(0,1 / 2)$。取序列 $\left(x_{n}\right) \in H$。取 $\delta<\left(\frac{\varepsilon}{b}\right)^{1 / \alpha}$ 知 $\left\{x_{n}\right\}$ 是等度连续的。进一步 $|x(t)| \leq b t^{\alpha}$ 所以这是有界的，然后由Arzela Ascoli这是准紧的。因此存在子序列同样记为 $x_{n}$ 一致收敛到某个 $x_{0}$ 。对于所有的 $s, t \in[0,1]$ 我们有
$$
\begin{aligned}
\left|x_{0}(s)-x_{0}(t)\right| & \leq\left|x_{0}(s)-x_{n}(s)\right|+\left|x_{0}(t)-x_{n}(t)\right|+\left|x_{n}(s)-x_{n}(t)\right| \\
& \leq\left|x_{0}(s)-x_{n}(s)\right|+\left|x_{0}(t)-x_{n}(t)\right|+b|s-t|^{\alpha} \\
& \rightarrow b|s-t|^{\alpha}
\end{aligned}
$$

所以$x_0\in H$，所以$H$是$C[0,1]$的紧子集。而上$\sup\{...\}<\infty$可转化为$\left.\bigcup_{n=1}^{\infty}\{\mid x(s)-x(t)\left|\leq n\right| s-\left.t\right|^{\alpha}\right\}$，每个$n$对应的集合是紧的，所以这个集合是可列个紧集的并。

## 3 统计决策理论

### 3.1 决策理论框架

决策理论将统计估计和推理构建为两人博弈，即自然与统计者的博弈。自然挑选 $\theta \in \Theta$，而 $\theta$ 的值对于统计学家是未知的；数据 $X$ 将会依 $\mathbb{P}_{\theta}$ 产生。 统计学家选择一个策略 $\delta$ 去依据$X$猜测 $\theta$ 的一些特征。数据产生后，统计学家应用这个规则并根据他的猜测$\delta(X)$以及真实值$\theta$支付损失 $\ell(\theta, \delta)$ 。这个博弈含有以下元素
(1) 可分巴拿赫空间$\mathcal{X}$上的概率分布集合 $\mathcal{P}=\left\{\mathbb{P}_{\theta}: \theta \in \Theta\right\}$ ，其中 $\Theta$ 是可分巴拿赫空间 $\mathcal{T}$ 的一个已知子集。 $\mathcal{P}$ 的元素对于自然可能的策略。
(2) 固定的由随机决策规则构成的集 $\mathcal{D}$，将 $\mathcal{X}$ 映射到行为空间 $\mathcal{A}$ 上概率分布。 $\mathcal{D}$ 中的元素为统计学家可用的策略。
(3) 损失函数 $\ell: \Theta \times \mathcal{A} \rightarrow \mathbb{R}^{+}$。 统计学家支付损失 $\ell(\theta, a)$ 若自然选择 $\theta$ 而统计学家采取行动 $a$。

基于数据$X \sim \mathbb{P}_{\theta}$，统计学家使用随机规则 $\delta \in \mathcal{D}$ 去选择行为，期望损失为在 $\theta \in \Theta$ 处决策规则 $\delta \in \mathcal{D}$ 的风险：
$$
r(\theta, \delta) \equiv \mathbb{E}_{\theta}\left[\int_{\mathcal{A}} \ell(\theta, a) \delta(X)(\mathrm{d} a)\right]
$$
当 $\delta$ 是非随机的，我们可以考虑 $\delta$ 在 $\mathcal{A}$ 中取值而不是在 $\mathcal{A}$ 上概率测度构成的集合
$$
r(\theta, \delta) \equiv \mathbb{E}_{\theta}[\ell(\theta, \delta(X))]
$$
统计学家寻求更聪敏地选择 $\delta \in \mathcal{D}$ 使得 $r(\theta, \delta)$ 尽可能小。



两个常用的选择最佳决策规则策略为

1. **极小化极大准则** 选择决策函数 $\delta \in \mathcal{D}$ 使得对于自然可能选择的参数 $\theta$ 最大风险最小。

**定义3.1.**  $\delta \in \mathcal{D}$ 在 $\Theta$ 上的最大风险为
$$
\rho(\delta) \equiv \sup _{\theta \in \Theta} r(\theta, \delta)
$$
则称极小化极大风险为
$$
\rho^{*}=\rho^{*}(\mathcal{D})=\inf _{\delta \in \mathcal{D}} \rho(\delta)
$$
若一个决策规则 $\delta^{*} \in \mathcal{D}$ 具有风险 $\rho\left(\delta^{*}\right)=\rho^{*}$ ，则称 $\delta^{*}$ 为极小化极大决策规则。



2. **贝叶斯准则** 先假设自然会根据先验分布 $\pi$ 从$\Theta$ 中随机抽取$\theta$，极小化带权重的风险。 

**定义 3.2.** 若 $\pi$ 是 $\Theta$ 上的概率测度，对于先验$\pi$，$\delta$ 的后验风险为
$$
\rho_{\pi}(\delta)=\int_{\mathcal{T}} r(\theta, \delta) \pi(\mathrm{d} \theta)
$$
则称最小后验风险为贝叶斯风险
$$
\rho_{\pi}^{*}=\inf _{\delta \in \mathcal{D}} \rho_{\pi}(\delta)
$$
若一个决策规则取得贝叶斯风险( if $\left.\rho_{\pi}\left(\delta^{*}\right)=\rho_{\pi}^{*}\right)$，则称其为对于先验 $\pi$ 的贝叶斯决策。

虽然统计学家也许不能找到极小化极大或贝叶斯决策规则，至少可以丢弃一个决策规则如果能找到另一个决策规则对所有$\theta \in \Theta$表现更好。



**定义3.3.** 称一个决策规则 $\delta$ 对于损失 $\ell$ 是可容许（admissible）的，如果不存在其他决策规则 $\delta^{\prime}$ 满足
$$
r\left(\theta, \delta^{\prime}\right) \leqslant r(\theta, \delta), \quad \forall \theta \in \Theta
$$
且至少存在一个 $\theta \in \Theta$ 有  $r\left(\theta, \delta^{\prime}\right)<r(\theta, \delta)$ for at least one $\theta \in \Theta$。若存在这样的 $\delta^{\prime}$ ，则称其dominate $\delta$。若 $\delta$ 不是可容许的则称其为不可容许的（inadmissible）。

**例子3.4.** 在线性反问题中，考虑估计线性独立线性泛函 $m$ -vector $g$ ，如定理2.7所描述；假设误差为Gaussian，且有如下定理条件：$g=\Lambda \cdot \kappa$ ，其中$\Lambda$ 为 $m \times n$ 矩阵。虽然 Backus-Gilbert 估计量 $\Lambda \cdot X$ 对于 $g$ 是无偏的。但是如果 $m \geqslant 3$ 且数据误差的协方差矩阵 $\Sigma$ 是满秩的，那么 $\Lambda \cdot X$ 对于MSE是不容许的。

但是，如果 $m<3$，则Backus-Gilbert估计量对于MSE为minimax，且能被表示为先验概率分布在$\mathbb{R}^{m}$中increasingly 'flat'的贝叶斯估计量的极限。



**定义3.5.** 一个统计量是从数据空间$\mathcal{X}$到某些其它可测空间的可测映射。一个统计量 $T$ 对于 $\mathcal{P}$ 是充分的，如果给定$T(X)$ ，便存在一个在$\mathbb{P}_{\theta}$下的$X$的条件分布不依赖于 $\theta \in \Theta$。$X$ 是 $\mathcal{P}$ 的充分统计量，这是平凡正确的。

对于凸损失函数，以下结果表明，充分统计量作为估计量，没有任何信息损失。

**定理3.6.** (Rao-Blackwell theorem (see [47, theorem 1.7.8])). 令 $X$ 有概率分布 $\mathbb{P}_{\theta} \in \mathcal{P}=\left\{\mathbb{P}_{\theta^{\prime}}: \theta^{\prime} \in \Theta\right\}$，且令 $T$ 对 $\mathcal{P}$ 充分。令 $\hat{g}$ 为参数 $g(\theta)$ 的估计量且令损失 $\ell(\theta, a)$ 在$a$中严格突。设 $\hat{g}(X)$ 对于所有的 $\mathbb{P}_{\theta}$ 可积，
$$
r(\theta, \delta)=\mathbb{E}_{\theta}[\ell(\theta, \hat{g}(X)]<\infty
$$
且
$$
\bar{g}(X)=\mathbb{E}_{\theta}[\hat{g}(X) \mid T(X)]
$$
（因为 $T(X)$ 是 $\theta$ 的充分统计量，右式的条件期望部依赖于 $\theta$ ）。那么
$$
r(\theta, \bar{g})<r(\theta, \hat{g})
$$
除非 $\hat{g}(X)=\bar{g}(X)$, $\mathbb{P}_{\theta}$ almost surely, 对于所有 $\theta \in \Theta$。



### 3.2. 决策估计

估计参数 $g(\theta)$ ， $g: \Theta \rightarrow \mathcal{G}$，其中 $\mathcal{G}$ 是一个范数为 $\|\cdot\|$ 的巴拿赫空间。取行动空间 $\mathcal{A}$ 为 $\mathcal{G}$ 考虑决策规则 $\delta$ 构成的集合 $\mathcal{D}$ ，其为从 $\mathcal{X}$ 到 $\mathcal{G}$ 的 $\mathbb{P}$ -measurable 映射。标准情况选取 $\ell(\theta, a)$ 为 $\|g(\theta)-a\|$，其为凸集。那么 $r(\theta, \delta)$ 为估计量的平均误差，在 $\mathcal{G}$ 的范数下的测度称为 MNE。 一个更少见的选择是 $\ell(\theta, a)=1_{g(\theta) \notin B_{c}(a)}$，其中 $B_{c}(a)=\{\eta \in \mathcal{G}:\|\eta-a\| \leqslant c\}$。当 $\mathcal{G}$ 是一个欧式空间，最常见的损失函数为 $\|a-g(\theta)\|^{2}$。当 $\mathcal{G}=\mathbb{R}$ （估计单个实参数），常见的损失函数为 $\ell(\theta, a)=|g(\theta)-a|^{p}$ 和 $\ell(\theta, a)=1_{|g(\theta)-a|>c}$。