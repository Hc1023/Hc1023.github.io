# 决策树

## 数据的表示

### 数据特征

算法如何“看”数据，**特征**是我们可以向这些数据提出的问题$f_1,...,f_n$，例如颜色、形状。

对分类问题，在学习/训练/归纳的时候，基于特征拟合一个分类器模型。模型可以根据特征对新的数据分类。

### 数据生成分布

- 我们要用**概率模型**去学习
- 数据/标签对服从某种概率分布，称为**数据的生成分布**

概率分布：描述某些事件的可能性，所有可能的结果及其发生的概率。

## 决策树

- 树的内部节点用特征作标签
- 树枝用是否符合特征来标签
- 叶子为类别标签

递归法：计算每个特征的“得分”，选择分数最高的特征，根据该特征的数值对数据进行划分并递归调用。

得分可以用训练误差（训练集上的平均误差率）衡量，对于分类问题，最常见的“误差”是错分的个数。

## 过拟合

递归法截止条件

基本情况：如果所有数据属于同一类，使用该标签创建叶节点或者所有数据有相同的特征。

**过拟合**发生在模型太过偏向训练数据时，目标是学习一个一般的模型，既要符合训练数据也要符合其他数据（比如测试数据）

### 阻止过拟合

递归法截止条件除基本情况外，还可以包括树达到一定深度、剩下一定数量/比例的数据、足够小训练误差、使用验证数据等。

- 修剪

树构建之后，返回并“修剪”树，即移除树的一些底层部分。类似于提前停止，但在整个树构建好以后完成。

- 规则：正则化方法

### 处理非二分值特征

- 多值特征：处理成多个划分，处理成多个二值划分
- 实值特征：用比较或范围划分

### 划分数据特征的得分

- 训练误差
- 基尼系数：划分后标签比例的平方和
- 熵：划分后标签分布的不确定性

敏感度：熵>基尼系数>训练误差

