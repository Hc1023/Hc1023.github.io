<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
</head>
<body>
<div id="TOC">
<ul>
<li><a href="#回归方法">回归方法</a><ul>
<li><a href="#逻辑斯蒂回归">1 逻辑斯蒂回归</a></li>
<li><a href="#判别分类">2 判别分类</a><ul>
<li><a href="#线性判别lda">2.1 线性判别（LDA）</a></li>
<li><a href="#二次判别分析qda">2.2 二次判别分析（QDA）</a></li>
</ul></li>
<li><a href="#k最近邻分类">3 K最近邻分类</a></li>
<li><a href="#决策树">4 决策树</a><ul>
<li><a href="#回归树">4.1 回归树</a></li>
<li><a href="#分类树">4.2 分类树</a></li>
</ul></li>
<li><a href="#集成学习">5 集成学习</a><ul>
<li><a href="#bagging">5.1 Bagging</a></li>
<li><a href="#随机森林">5.2 随机森林</a></li>
<li><a href="#adaboost">5.3 Adaboost</a></li>
</ul></li>
</ul></li>
</ul>
</div>
<h1 id="回归方法">回归方法</h1>
<h2 id="逻辑斯蒂回归">1 逻辑斯蒂回归</h2>
<p>因变量是定性变量（假设只取两个值），自变量可以定量、定性（哑变量）。输出结果在0到1之间，用逻辑斯蒂函数</p>
<p><span class="math display">\[
p(X)=\frac{e^{\beta_{0}+\beta_{1} X}}{1+e^{\beta_{0}+\beta_{1} X}}
\]</span></p>
<p>逻辑斯蒂函数可以等价地写成 <span class="math display">\[
\frac{p(X)}{1-p(X)}=e^{\beta_{0}+\beta_{1} X}
\]</span> 称左边为“发生比&quot;， 它的取值范围为 <span class="math inline">\((0, \infty)\)</span> 取对数，得: <span class="math display">\[
\log \left(\frac{p(X)}{1-p(X)}\right)=\beta_{0}+\beta_{1} X \text { . }
\]</span> 极大似然估计参数，似然函数： <span class="math display">\[
L\left(\beta_{0}, \beta_{1}\right)=\prod_{i: y_{i}=1} p\left(x_{i}\right) \prod_{i^{\prime}: y_{i^{\prime}}=0}\left(1-p\left(x_{i^{\prime}}\right)\right)
\]</span> 梯度下降法或牛顿迭代法求系数估计。</p>
<h2 id="判别分类">2 判别分类</h2>
<p>贝叶斯定理： <span class="math display">\[
p_{k}(x):=P(Y=k \mid X=x)=\frac{\pi_{k} f_{k}(x)}{\sum_{l=1}^{K} \pi_{l} f_{l}(x)}
\]</span> 估计<span class="math inline">\(\pi_k\)</span>：取随机样本，计算属于第<span class="math inline">\(k\)</span>类的样本占总样本的比例。</p>
<p>将一个待判别的<span class="math inline">\(x\)</span>分类到使得<span class="math inline">\(p_k(x)\)</span>达到最大的那个类，这种方法被称为贝叶斯分类器。</p>
<h3 id="线性判别lda">2.1 线性判别（LDA）</h3>
<p>假设预测变量只有1个。此外，假设 <span class="math inline">\(f_{k}(x)\)</span> 是正态的: <span class="math display">\[
f_{k}(x)=\frac{1}{\sqrt{2 \pi} \sigma_{k}} \exp \left\{-\frac{1}{2 \sigma_{k}^{2}}\left(x-\mu_{k}\right)^{2}\right\}, \quad x \in R
\]</span> 再假设 <span class="math inline">\(\sigma_{1}^{2}=\cdots=\sigma_{K}^{2}\)</span>, 简记为 <span class="math inline">\(\sigma^{2}\)</span>, 那么: <span class="math display">\[
p_{k}(x)=\frac{\pi_{k} \frac{1}{\sqrt{2 \pi} \sigma} \exp \left\{-\frac{1}{2 \sigma^{2}}\left(x-\mu_{k}\right)^{2}\right\}}{\sum_{l=1}^{K} \pi_{l} \frac{1}{\sqrt{2 \pi} \sigma} \exp \left\{-\frac{1}{2 \sigma^{2}}\left(x-\mu_{l}\right)^{2}\right\}}
\]</span> 对 <span class="math inline">\(p_{k}(x)\)</span> 两边取对数，可知贝叶斯分类器其实是将观测x分到使得 <span class="math display">\[
\delta_{k}(x)=x \frac{\mu_{k}}{\sigma^{2}}-\frac{\mu_{k}^{2}}{2 \sigma^{2}}+\log \pi_{k}
\]</span> 达到最大的那一类。 假设<span class="math inline">\(K =2,\ \pi_{1}=\pi_{2}\)</span>, 则当 <span class="math inline">\(2x\left(\mu_{1}-\mu_{2}\right)&gt;\mu_{1}{ }^{2}-\mu_{2}{ }^{2}\)</span> 时，贝叶斯分类器将 观测分入类1, 否则分入类2. 贝叶斯决策边界: <span class="math display">\[
x=\frac{\mu_{1}^{2}-\mu_{2}^{2}}{2\left(\mu_{1}-\mu_{2}\right)}=\frac{\mu_{1}+\mu_{2}}{2} \text { . }
\]</span></p>
<h3 id="二次判别分析qda">2.2 二次判别分析（QDA）</h3>
<p>假设来自第k类的随机观测服从<span class="math inline">\(N\left(\mu_{k}, \sigma_{k}^{2}\right)\)</span>, 在这种假设下，贝叶斯 分类器把观测分入使得 <span class="math display">\[
\begin{aligned}
\delta_{k}(x) &amp;=-\frac{1}{2 \sigma_{k}^{2}}\left(x-\mu_{k}\right)^{2}+\log \pi_{k} \\
&amp;=-\frac{1}{2 \sigma_{k}^{2}} x^{2}+\frac{x \mu_{k}}{\sigma_{k}^{2}}-\frac{\mu_{k}^{2}}{2 \sigma_{k}^{2}}+\log \pi_{k}
\end{aligned}
\]</span> 达到最大的那一类。</p>
<p>当有<span class="math inline">\(p\)</span>个自变量时, LDA的协方差矩阵有<span class="math inline">\(p(p+1)/2\)</span>个参数, 而QDA的<span class="math inline">\(K\)</span>个协方差矩阵有<span class="math inline">\(Kp(p+1)/2\)</span>个参数。所以LDA没有QDA分类器光滑，LDA拥有更小的方差和更大的预测偏差。样本量小，用LDA；样本量多，用QDA。</p>
<h2 id="k最近邻分类">3 K最近邻分类</h2>
<p>对新的观测，根据其<span class="math inline">\(k\)</span>个最近邻的训练数据的类别，通过多数表决等方式进行类别预判。因此，<span class="math inline">\(k\)</span>最近邻方法不具有显式学习过程。</p>
<p><span class="math inline">\(k\)</span>最近邻法的三个基本要素：</p>
<ul>
<li><span class="math inline">\(k\)</span>的选择</li>
<li>距离度量</li>
<li>决策规则</li>
</ul>
<p>涵盖最邻近<span class="math inline">\(k\)</span>个点的 <span class="math inline">\(x\)</span> 的邻域记作 <span class="math inline">\(N_{k}(x)\)</span> ，在 <span class="math inline">\(N_{k}(x)\)</span> 中根据分类决策规则(如多数表决)决定 <span class="math inline">\(x\)</span> 的类別 <span class="math inline">\(y\)</span> <span class="math display">\[
y=\arg \max _{c_{j}} \sum_{x_{i} \in N_{k}(x)} I\left\{y_{i}=c_{j}\right\}, \quad i=1, \cdots, N ; j=1, \cdots, L ;
\]</span> 当<span class="math inline">\(k=1\)</span>时，最近邻分类器偏差较小但方差很大，决策边界很不规则。 当<span class="math inline">\(k\)</span>变大时，方差较低但偏差却增大，将得到一个接近线性的决策边界。 在实际中，可用交叉验证的方法选择<span class="math inline">\(k\)</span>的大小。</p>
<h2 id="决策树">4 决策树</h2>
<p>分而治之</p>
<p>因变量数值型 - 回归问题 - 回归树</p>
<p>因变量类别型 - 分类问题 - 分类树</p>
<ul>
<li>模型具有可读性</li>
<li>预测的速度快</li>
</ul>
<h3 id="回归树">4.1 回归树</h3>
<p>出于简化模型和增加模型的可解释性的考虑，通常将自变量空间划为高维矩形，或称为盒子。划分区域的目标是找到使模型的残差平方和RSS最小的矩形区域 $R_{1}, , R_{J} $. RSS的定义是: <span class="math display">\[
\sum_{j=1}^{J} \sum_{i \in R_{j}}\left(y_{i}-\hat{y}\left(R_{j}\right)\right)^{2}
\]</span> 其中, <span class="math inline">\(\hat{y}\left(R_{j}\right)\)</span> 是第<span class="math inline">\(j\)</span>个矩形区域中训练观测的平均响应值。</p>
<p>将自变量空间划分为<span class="math inline">\(J\)</span>个矩形区域，一般采用自上而下、贪婪的方法：递归二叉分裂。</p>
<p>在执行递归二叉分裂时，先选择自变量<span class="math inline">\(X_{j}\)</span> 和分割点<span class="math inline">\(s\)</span>，将自变量空间分为两个区域: <span class="math display">\[
\begin{array}{l}
\left\{X \mid X_{j}&lt;s\right\} \\
\left\{X \mid X_{j} \geq s\right\}
\end{array}
\]</span> 选择出来的两个区域要能够使RSS尽可能地减少。</p>
<p>重复至某个停止标准，如所有区域包含的观测个数都不大于5.</p>
<ul>
<li>过拟合 -&gt; 剪枝</li>
</ul>
<p><strong>成本复杂性剪枝</strong></p>
<p>不是考虑每一棵可能的子树，而是考虑以非负调节参数 <span class="math inline">\(\alpha\)</span> 标记的一列子树。每一个 <span class="math inline">\(\alpha\)</span> 的取值对应一棵子树 <span class="math inline">\(T \subset T_{0}\)</span> 。当 <span class="math inline">\(\alpha\)</span> 值给定时，其对应的子树需使下式最小 <span class="math display">\[
\sum_{m=1}^{|T|} \sum_{i: \boldsymbol{x}_{i} \in R_{m}}\left(y_{i}-\hat{y}\left(R_{m}\right)\right)^{2}+\alpha|T|
\]</span> 这里的<span class="math inline">\(|T|\)</span>表示树的叶节点个数， <span class="math inline">\(R_m\)</span> 是第<span class="math inline">\(m\)</span>个叶节点对应的盒子, <span class="math inline">\(\hat{y}\left(R_{m}\right)\)</span> 是相应的响应预测值。</p>
<p>交叉验证挑选<span class="math inline">\(\alpha\)</span>，使均方预测误差达到最小，从而确定最优子树。</p>
<h3 id="分类树">4.2 分类树</h3>
<p>递归二叉分裂。RSS的替代指标：</p>
<ul>
<li>分类错误率</li>
</ul>
<p><span class="math display">\[
E_{m}=1-\max _{k} \hat{p}_{m k}
\]</span> 其中， <span class="math inline">\(\hat{p}_{\mathrm{mk}}\)</span> 表示第<span class="math inline">\(m\)</span>个区域的训练观测中第<span class="math inline">\(k\)</span>类所占的比例， <span class="math inline">\(\max _{k} \hat{p}_{m k}\)</span> 是分类正确率。</p>
<p>但分类错误率在分类树的构建中不够敏感，常用基尼指数和互熵衡量节点纯度。</p>
<ul>
<li>基尼指数</li>
</ul>
<p><span class="math display">\[
G_{m}=\sum_{k=1}^{K} \hat{p}_{m k}\left(1-\hat{p}_{m k}\right) .
\]</span></p>
<ul>
<li>互熵</li>
</ul>
<p><span class="math display">\[
D_{m}=-\sum_{k=1}^{K} \hat{p}_{m k} \log \hat{p}_{m k}
\]</span></p>
<h2 id="集成学习">5 集成学习</h2>
<ul>
<li><p>构建并整合多棵分类树</p></li>
<li><p>个体分类树应“好而不同”</p></li>
</ul>
<p>两大类集成树的产生方法：</p>
<ul>
<li>个体分类树之间不存在强依赖关系、同时生成的并行方法，如Bagging和随机森林</li>
<li>个体分类树之间存在强依赖关系、串行生成的序列化方法，如Boosting（Adaboost）</li>
</ul>
<h3 id="bagging">5.1 Bagging</h3>
<p>Bagging主要关注降低预测模型的方差。 给定<span class="math inline">\(n\)</span>个独立随机变量<span class="math inline">\(Z_{1}, \cdots, Z_{n}\)</span>, 假设它们的方差都为 <span class="math inline">\(\sigma^{2}\)</span>, 那么样本 均值 <span class="math inline">\(\bar{Z}=\frac{1}{n} \sum_{i=1}^{n} Z_{i}\)</span> 的方差为 <span class="math inline">\(\sigma^{2} / \mathrm{n}_{\text {。 }}\)</span> 启发：从总体中抽取多个训练集，对每个训练集分别建立预测模型，再对由此得到的全部预测模型求平均，从而降低方差，得到一个集成模型。</p>
<p>即，可以用B个独立的训练集训练出B个模型: <span class="math inline">\(\hat{f}^{1}(x), \cdots, \hat{f}^{B}(x)\)</span>, 然 后求平均，得到一个低方差的模型: <span class="math display">\[
\hat{f}_{\text {avg }}(\boldsymbol{x})=\frac{1}{B} \sum_{b=1}^{B} \hat{f}^{b}(\boldsymbol{x}) .
\]</span> 在实际中，不容易得到多个训练集。自助抽样法(Bootstrap)可以解决这个问题。</p>
<p><span class="math inline">\(B\)</span>足够大，可稳定预测准确率。</p>
<p>可以对某一自变量在一棵个体分类上因分裂导致的基尼指数减少量加总，再在所有<span class="math inline">\(B\)</span>棵个体分类树上求平均值。平均值越大就说明这个自变量越重要。</p>
<h3 id="随机森林">5.2 随机森林</h3>
<p>以决策树为基础构建Bagging分类树的基础上，进一步在决策树的训练过程中引入了<strong>自变量的随机选择</strong>，从而达到对树的去相关（decorrelating），实现对Bagging的改进。</p>
<p>在建立这些个体分类树时，每考虑树上的一个分裂点，都要从全部 的p个自变量中选出一个包含<span class="math inline">\(q\ (1 \leq q \leq p)\)</span>个自变量的随机样本作为候选变量。这个分裂点所用的自变量只能从这<span class="math inline">\(q\)</span>个变量中选择。在每个分裂点处都重新进行抽样，选出<span class="math inline">\(q\)</span>个自变量。若<span class="math inline">\(q=p\)</span>，则随机森林就是Bagging. 通常取<span class="math inline">\(q\)</span>为<span class="math inline">\(p\)</span>的平方根。</p>
<p>若个体分类树有高度相关性，对高度相关的变量求平均无法大幅度减少方差。而随机森林对不相关变量求平均，可大幅度降低方差。</p>
<p><strong>样本多样性来自</strong></p>
<ul>
<li><p>Bagging：样本扰动</p></li>
<li><p>随机森林：样本扰动+自变量扰动</p></li>
</ul>
<h3 id="adaboost">5.3 Adaboost</h3>
<p>Boosting：可将弱分类器提升为强分类器。根据分类器的表现对训练样本分布进行调整，使先前分类器错分的训练样本在后续得到更多的关注。</p>
<p>Adaboost是Boosting中的一种，算法：</p>
<p><strong>输入</strong>：训练集 <span class="math inline">\(D=\left\{\left(\boldsymbol{x}_{1}, y_{1}\right), \cdots,\left(\boldsymbol{x}_{m}, y_{m}\right)\right\}\)</span>; 分类器算法 <span class="math inline">\(\mathfrak{L} ;\)</span> 训练轮数<span class="math inline">\(T\)</span>;</p>
<p><strong>过程</strong>：</p>
<p>(a)<span class="math inline">\(\mathcal{D}_{1}(\boldsymbol{x})=1 / m\)</span>. (b) 对 <span class="math inline">\(t=1, \cdots, T\)</span>, 执行: (c) <span class="math inline">\(h_{t}=\mathfrak{L}\left(D, \mathcal{D}_{t}\right)\)</span> (d) <span class="math inline">\(\epsilon_{t}=P_{\boldsymbol{x} \sim \mathcal{D}_{t}}\left(h_{t}(\boldsymbol{x}) \neq f(\boldsymbol{x})\right)\)</span>; (e) 如果 <span class="math inline">\(\epsilon_{t}&gt;0.5\)</span>, 则停止；否则，继续执行; (f) <span class="math inline">\(\alpha_{t}=\frac{1}{2} \ln \left(\frac{1-\epsilon_{t}}{\epsilon_{t}}\right)\)</span>; (g) 令 <span class="math display">\[
\begin{aligned}
\mathcal{D}_{t+1}(\boldsymbol{x}) &amp;=\frac{\mathcal{D}_{f}(\boldsymbol{x}) \exp \left(-\alpha_{t} f(\boldsymbol{x}) h_{t}(\boldsymbol{x})\right)}{Z_{t}} \\
&amp;=\frac{\mathcal{D}_{t}(\boldsymbol{x})}{Z_{t}} \times\left\{\begin{array}{ll}
\exp \left(-\alpha_{t}\right), &amp; \text { 如果 } h_{t}(\boldsymbol{x})=f(\boldsymbol{x}) \\
\exp \left(\alpha_{t}\right), &amp; \text { 如果 } h_{t}(\boldsymbol{x}) \neq f(\boldsymbol{x})
\end{array}\right.
\end{aligned}
\]</span> 其中 <span class="math inline">\(Z_{t}\)</span> 是归一化常数; (h) 循环结束.</p>
<p><strong>输出</strong>： <span class="math inline">\(H(\boldsymbol{x})=\operatorname{sign}\left(\sum_{t=1}^{T} \alpha_{t} h_{t}(\boldsymbol{x})\right)\)</span>.</p>
<p>从偏差-方差权衡角度，Adaboost更关注降低偏差。</p>
</body>
</html>
