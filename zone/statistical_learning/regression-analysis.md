[Hc's blog](https://hc1023.github.io/)

[Hc's blog: R与统计学习基础](https://hc1023.github.io/2021/06/13/Statistical-learning/)

[机器学习与人工智能 浙江大学 中国大学MOOC(慕课)](https://www.icourse163.org/course/ZJU-1206689820)

[TOC]

# 回归分析

回归分析是使用方程来表达感兴趣的变量（称为因变量）与一系列相关变量（称为自变量）之间关系的分析过程。

“相关关系”的变量：不能用函数刻画，但具有一定的“趋势性”关系的变量。

回归分析研究对象：具有相关关系的变量。研究目的：寻找它们之间客观存在的依赖关系。

## 线性回归

**回归分析模型**

假设因变量$y$的取值可看成由两部分组成：由自变量$x$决定的部分（记为$f(x)$）以及其它未加考虑的因素所产生的影响（称为随机误差，记作$\varepsilon$）。因此有下列模型：
$$
y=f(x)+\varepsilon.
$$
若$f(x)=a+bx$，则$y=a+bx+\varepsilon$，其中，回归常数$a$，回归系数$b$，有时统称为回归系数。

假设$E(\varepsilon)=0$，称$E(y|x)=f(x)=a+bx$为回归函数。刻画了在平均意义下因变量与自变量的相应关系。

回归分析的首要问题是**估计回归函数**，也就是估计回归系数。

最小二乘法：$\sum_{i=1}^{n}\left(y_{i}-a-b x_{i}\right)^{2}$.

最小二乘估计：$\left\{\begin{array}{l}\hat{a}=\bar{y}-\hat{b} \bar{x}, \\ \hat{b}=\frac{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)\left(y_{i}-\bar{y}\right)}{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}} .\end{array}\right.$其中，$\bar{y}=\frac{1}{n} \sum_{i=1}^{n} y_{i}, \quad \bar{x}=\frac{1}{n} \sum_{i=1}^{n} x_{i}$.

推广: 多元线性回归模型
$$
y=\beta_{0}+\beta_{1} x_{1}+\cdots+\beta_{p} x_{p}+\varepsilon.
$$
历史数据
$$
\left(x_{i 1}, \cdots, x_{i p}, y_{i}\right), i=1, \cdots, n.
$$
回归方程:
$$
\hat{y}=\hat{\beta}_{0}+\hat{\beta}_{1} x_{1}+\cdots+\hat{\beta}_{p} x_{p}.
$$
回归方程应用：

- 定量刻画自变量与因变量的相依关系（正相依或负相依）；
- 给定自变量的取值，用回归方程预测相应的因变量的取值。

线性模型易于描述、容易实现，统计推断的理论成果也相对成熟。

线性模型的假设：

- 因变量与自变量之间的相依关系是线性的：无论自变量$x_j$取何值，它变化一个单位所引起的因变量 $y$
  的变化大小是恒定的；
- 因变量与自变量之间的关系是加性的：自变量$x_j$的变化对因变量 $y$的影响与其它自变量的取值无关。

## 多项式回归

实际中，自变量与因变量的关系不满足线性假设和加性假设。

采用多项式回归放宽自变量与因变量之间的线性假设：
$$
y=\beta_{0}+\beta_{1} x+\cdots+\beta_{t} x^{t}+\varepsilon.
$$
实际应用中，对多项式阶数$t$的选择不宜过大，一般不大于3或者4. 这是因为$t$越大，多项式曲线就会越曲折，在$x$的取值的边界处会呈现异样的形状。

实际中，自变量不会独立影响，即不满足加性假设，从而引入**交互项**，例如：
$$
y=\beta_{0}+\beta_{1} x_{1}+\beta_{2} x_{2}+\beta_{3} x_{1} x_{2}+\varepsilon
$$
$x_{1} x_{2}$：交互项，它反应了$x_1$和$x_{2}$对$y$的交互效应。
改写模型
$$
\begin{aligned}
y &=\beta_{0}+\left(\beta_{1}+\beta_{3} x_{2}\right) x_{1}+\beta_{2} x_{2}+\varepsilon \\
&=\beta_{0}+\tilde{\beta}_{1} x_{1}+\beta_{2} x_{2}+\varepsilon,
\end{aligned}
$$
其中
$$
\tilde{\beta}_{1}=\beta_{1}+\beta_{3} x_{2}.
$$
因为$\tilde{\beta}_1$随$x_{2}$变化，所以$x_{1}$对$y$的边际效应不再是常数，即，调整$x_{2}$的值将改变$x_{1}$对$y$的影响。

## 样条回归

前文多项回归可看成是一种特殊的基函数回归方法。

基函数回归的基本原理是对自变量$x$的函数或变换$b_1(x),...,b_t(x)$进行回归建模，以如下模型来替代普通的线性模型
$$
y=\beta_{0}+\beta_{1} b_{1}(x)+\cdots+\beta_{t} b_{t}(x)+\varepsilon.
$$
对于多项式回归，基函数为$b_{j}(x)=x^{j}$. 样条回归也是基函数回归。

分段多项式回归的基本思想：在自变量$x$的不同取值区域各自拟合低阶的多项式函数。

例如，分段三次多项式回归在x的不同取值区域分别拟合如下模型:
$$
y=\beta_{0}+\beta_{1} x+\beta_{2} x^{2}+\beta_{3} x^{3}+\varepsilon,
$$
四个回归系数在$x$的不同取值区域不必都相同。回归系数发生变化的（$x$的）临界点称为结点（knot）。

假设$K$个结点，拟合$K+1$个不同的三次多项式，并在结点处添加连续性约束和光滑性（一阶导数和二阶导数）约束。

**基函数回归方法**

一个带有$K$个结点的三次**样条回归**模型可以表示为：
$$
y=\beta_{0}+\beta_{1} b_{1}(x)+\cdots+\beta_{K+3} b_{K+3}(x)+\varepsilon.
$$

对上述模型中，以三次多项式的基( $x$，$x$的平方，$x$的立方 $)$ 为基础，然后在每 个结点处添加一个截断幂基:
$$
h(x, \xi)=\left\{\begin{array}{ll}
(x-\xi)^{3}, & x>\xi, \\
0, & x \leq \xi,
\end{array}\right.
$$
其中，$\xi$是结点。

拟合带有$K$个结点的三次样条，只需把$x, x^{2}, x^{3}, h\left(x, \xi_{1}\right), \cdots, h\left(x, \xi_{K}\right)$作为自变量来建立回归模型， $\xi_{1}, \cdots, \xi_{k}$是结点。

但是样条在边界处方差较大，自然样条可以缓解这个问题。**自然样条**是附加了边界约束的样条回归：回归函数在边界区域是线性的。这个附加的约束使得自然样条在边界处产生更稳定的估计。

- 设置多少结点？若结点个数过多，曲线非常曲折；反之，结点个数过少，回归曲线过于平坦。实际采用交叉验证法。
- 结点选在什么位置？实践证明令结点在数据上呈现均匀分布是一种行之有效的方式。

**光滑样条回归**

给定训练数据，想拟合一条光滑回归曲线，需要找到某个函数，记为$g(x)$，使它与训练数据能很好地吻合，即，使误差平方和尽可能小
$$
\sum_{i=1}^{n}\left(y_{i}-g\left(x_{i}\right)\right)^{2}.
$$
后果：若我们选择$g$在每个样本点处做插值，就可得到一个取值为0的误差平方和。过拟合，欠光滑。实际上，真正需要的$g$是能够让误差平方和尽可能小的同时也要让回归曲线尽量光滑。

其中一种方法是最小化以下的“损失+惩罚”函数
$$
\sum_{i=1}^{n}\left(y_{i}-g\left(x_{i}\right)\right)^{2}+\lambda \int\left[g^{\prime \prime}(t)\right]^{2} d t,
$$
其中， $\lambda$ 是一个非负的调节参数，实际应用中通过交叉验证法选择$\lambda$的大小。通过最小化上述目标函数得到的$g$就是光滑样条。最小化上面这个“损失+惩罚”函数，得到函数$g$的性值：在不同$x_1,...,x_n$处，$g$是带结点三次多项式函数；在每个结点处的一阶导数和二阶导数是连续的；在两个边界结点之外的区域是线性的。也就是$g$是带结点的自然三次样条。

## 局部回归

局部回归方法是一种非参数回归的方法。非参数回归模型（事先不设定回归模型的具体形式）的拟合方法可分为两类：

- 基于基函数逼近的整体型方法
- 基于光滑思想的局部拟合方法

非参数回归模型的一般形式：$y=m(x)+\varepsilon$，对函数$m$只做一些连续性或光滑性的要求。

**Nadaraya-Watson估计方法**

N-W方法：对于任意给定的$x_0\in \mathcal D$（$\mathcal D$是自变量$x$的取值范围)，通过一个合适的函数（称为核函数）利用自变量的观测值在$x_0$处产生权值（越靠近$x_0$，权值越大），基于权值对因变量的观测值进行加权平均，得到回归函数$m(x)$在$x_0$处的估计。

其中，加权平均的范围和权值由一个称为带宽的参数所控制。

具体方法：设$K(t)$为给定的核函数，对称，单峰，且满足
$$
\lim_{|t|\rightarrow\infty}K(t)=0,
$$
记$K_h(t)=\frac 1 h K(\frac t h)$，其中$h>0$为带宽。

N-W估计：
$$
\hat m_{NW}(x_0)=\frac{\sum_{i=1}^n K_h(x_i-x_0)y_i}{\sum_{i=1}^nK_h(x_i-x_0)}=\frac{\sum_{i=1}^n K(\frac{x_i-x_0}{h})y_i}{\sum_{i=1}^nK(\frac{x_i-x_0}{h})}.
$$
带宽$h$的大小对回归函数的估计有重要影响：

- 当$h$接近0时，$\hat m_{NW}(x_j)\approx K(0)y_j/K(0)=y_j$. 估计的回归函数波动越大，从而导致过拟合。

- 当$h$很大时，$\hat m_{NW}(x_j)\approx\sum_{i=1}^n K(0)y_i/\sum_{i=1}^n K(0)=\frac 1 n \sum_{i=1}^n y_i = \bar y$. 估计的回归函数曲线越光滑，从而导致欠拟合。

在实际中用交叉验证方法选择合适的$h$大小。

**常用的核函数**

- Gauss核

$$
K(t)=\frac{1}{\sqrt{2\pi}}\exp (-\frac 1 2 t^2).
$$

- 对称Beta函数族

$$
K(t)=\frac{1}{\text{Beta}(1/2,\gamma+1)}(1-t^2)_{+}^{\gamma},\ \gamma=0,1,2,...
$$

N-W估计可看成是下面的加权最小二乘问题：
$$
\min_{a(x_0)}\sum_{i=1}^n(y_i-a(x_0))^2K_h(x_i-x_0).
$$
N-W估计其实是将回归函数在每一点的局部视为常数，然后通过加权最小二乘方法得到回归函数在该点处的估计。N-W估计也被称为局部常数估计。

**局部多项式光滑方法**

要得到更优的估计，可以在每一点的局部，用$p$次多项式逼近回归函数，然后基于加权最小二乘方法得到回归函数在各点的估计。

设$m(x)$有$p$阶连续导数，对于任意给定的$x_0\in\mathcal{D}$，由Taylor公式，在$x_0$的邻域内
$$
\begin{aligned}
m(x)&\approx m(x_0)+m'(x_0)(x-x_0)+\frac{m''(x_0)}{2}(x-x_0)^2+...+\frac{m^{(p)}(x_0)}{p!}(x-x_0)^p\\
&=\sum_{j=0}^p \beta_j(x_0)(x-x_0)^j,
\end{aligned}
$$
其中，$\beta_j(x_0)=m^{(j)}(x_0)/j!,\ j=0,1,...,p$.

局部多项式估计利用加权最小二乘方法在$x_0$的局部拟合上述多项式，
$$
\hat\beta_0(x_0)=\hat m(x_0)
$$
作为回归函数$m(x_0)$在$x_0$处的估计值。

目标函数：
$$
\sum_{i=1}^n\left(y_i-\sum_{j=0}^p\beta_j(x_0)(x_i-x_0)^j\right)^2K_h(x_i-x_0).
$$
在R中使用`loess`进行局部多项式回归拟合。

## 广义加性模型

广义加性模型（generalized additive model，GAM）提供了一个推广标准线性回归模型的一般框架。在这个框架里，每一个自变量都被一个它的非线性函数所取代，同时仍保持自变量的可加性。

模型：$y=\beta_0+f_1(x_2)+...+f_p(x_p)+\varepsilon$.

将基于自变量$x_j$的样本数据独立拟合$f_j,\ j=1,...,p$，然后再把$p$个拟合出来的函数进行加总。拟合方法有样条方法、多项式回归、局部回归等。（注意前文$x_i$表示不同的样本点，这里$x_p$表示不同的自变量）

优点：

- 可自动对自变量和因变量进行非线性关系的建模；
- 非线性拟合可能会提高对因变量的预测精度；
- 因为加性，在保持其它自变量不变的情形下可以分析每个自变量对因变量的单独效应。

广义加性模型的主要缺点也在“加性”，它忽略了自变量之间的交互效应。

注意：要想摆脱广义加性模型在模型形式上的设定缺陷，可使用随机森林等更一般的方法。广义加性模型可以视为介于线性模型和完全非参数模型之间的一类折中的方法。



